{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example: Latch Problem</h1>\n",
    "\n",
    "We study an easy example of learning long-term dependencies by using a simple <i>latch task</i> (see [Hochreiter and Mozer](https://link.springer.com/chapter/10.1007/3-540-44668-0_92)). The essence of this task is that a sequence of inputs is presented, beginning with one of two symbols, <b>A</b> or <b>B</b>, and after a variable number of time steps, the model has to output a corresponding symbol. Thus, the task requires memorizing the original input over time. It has to be noted, that both class-defining symbols must only appear at the first position of a sequence. This task was specifically designed to demonstrate the capability of recurrent neural networks to capture long term dependencies. This demonstration shows, that <code>Hopfield</code>, <code>HopfieldPooling</code> and <code>HopfieldLayer</code> adapt extremely fast to this specific task, concentrating only on the first entry of the sequence.\n",
    "\n",
    "This demonstration instructs how to apply <code>Hopfield</code>, <code>HopfieldPooling</code> and <code>HopfieldLayer</code> for an exemplary sequential task, potentially substituting LSTM and GRU layers.\n",
    "\n",
    "NOTA BENE: No tweeking of the exemplary LSTM network is done. The focus is put on the technical details. Feel free to tune yourself and see what works better :)\n",
    "\n",
    "<h3 style=\"color:rgb(208,90,80)\">In the chapters <a href=\"#Adapt-Hopfield-based-Network\">Adapt Hopfield-based Network</a>, <a href=\"#Adapt-Hopfield-based-Pooling\">Adapt Hopfield-based Pooling</a> and <a href=\"#Adapt-Hopfield-based-Lookup\">Adapt Hopfield-based Lookup</a> you can explore and try the new functionalities of our new Hopfield layer.</h3>\n",
    "\n",
    "In order to run this notebook, a few modules need to be imported. The installation of third-party modules is <i>not</i> covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_497130/2319755973.py:14: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils.version import LooseVersion\n"
     ]
    }
   ],
   "source": [
    "# Import general modules used e.g. for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Importing Hopfield-specific modules.\n",
    "from hflayers import Hopfield, HopfieldPooling, HopfieldLayer\n",
    "from hflayers.auxiliary.data import LatchSequenceSet\n",
    "\n",
    "# Import auxiliary modules.\n",
    "from distutils.version import LooseVersion\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Importing PyTorch specific modules.\n",
    "from torch import Tensor\n",
    "from torch.nn import Flatten, Linear, LSTM, Module, Sequential\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Set plotting style.\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific minimum versions of Python itself as well as of some used modules is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version:  3.10.12 (✓)\n",
      "Installed PyTorch version: 2.0.1+cu117 (✓)\n"
     ]
    }
   ],
   "source": [
    "python_check = '(\\u2713)' if sys.version_info >= (3, 8) else '(\\u2717)'\n",
    "pytorch_check = '(\\u2713)' if torch.__version__ >= LooseVersion(r'1.5') else '(\\u2717)'\n",
    "\n",
    "print(f'Installed Python version:  {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro} {python_check}')\n",
    "print(f'Installed PyTorch version: {torch.__version__} {pytorch_check}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Create Dataset</h3>\n",
    "\n",
    "We study an easy example of learning long-term dependencies by using a simple <i>latch task</i>. \n",
    "The latch task was introcuded by Hochreiter and Mozer:<br>\n",
    "<cite>Sepp Hochreiter, Michael Mozer, 2001. A discrete probabilistic memory model for discovering dependencies in time. Artificial Neural Networks -- ICANN 2001, 13, pp.661-668.</cite><br><br>\n",
    "The essence of this task is that a sequence of inputs is presented, beginning with one of two symbols, <b>A</b> or <b>B</b>, and after a variable number of time steps, the model has to output a corresponding symbol. Thus, the task requires memorizing the original input over time. It has to be noted, that both class-defining symbols must only appear at the first position of an instance. Defining arguments are:\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_samples</code></th>\n",
    "        <th>4096</th>\n",
    "        <th>Amount of samples of the full dataset.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_instances</code></th>\n",
    "        <th>32</th>\n",
    "        <th>Amount of instances per sample (sample length).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_characters</code></th>\n",
    "        <th>20</th>\n",
    "        <th>Amount of different characters (size of the one-hot encoded vector).</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Let's define the dataset using previously mentioned properties as well as a logging directory for storing all auxiliary outputs like performance plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latch_sequence_set = LatchSequenceSet(\n",
    "    num_samples=4096,\n",
    "    num_instances=32,\n",
    "    num_characters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f'resources/'\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Create Auxiliaries</h3>\n",
    "\n",
    "Before digging into Hopfield-based networks, a few auxiliary variables and functions need to be defined. This is nothing special with respect to Hopfield-based networks, but rather common preparation work of (almost) every machine learning setting (e.g. definition of a <i>data loader</i> as well as a <i>training loop</i>). We will see, that this comprises the most work of this whole demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(r'cuda:0' if torch.cuda.is_available() else r'cpu')\n",
    "\n",
    "# Create data loader of training set.\n",
    "sampler_train = SubsetRandomSampler(list(range(512, 4096 - 512)))\n",
    "data_loader_train = DataLoader(dataset=latch_sequence_set, batch_size=32, sampler=sampler_train)\n",
    "\n",
    "# Create data loader of validation set.\n",
    "sampler_eval = SubsetRandomSampler(list(range(512)) + list(range(4096 - 512, 4096)))\n",
    "data_loader_eval = DataLoader(dataset=latch_sequence_set, batch_size=32, sampler=sampler_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network: Module,\n",
    "                optimiser: AdamW,\n",
    "                data_loader: DataLoader\n",
    "               ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Execute one training epoch.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader: data loader instance providing training data\n",
    "    :return: tuple comprising training loss as well as accuracy\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    losses, accuracies = [], []\n",
    "    for sample_data in data_loader:\n",
    "        data, target = sample_data[r'data'], sample_data[r'target']\n",
    "        data, target = data.to(device=device), target.to(device=device)\n",
    "\n",
    "        # Process data by Hopfield-based network.\n",
    "        model_output = network.forward(input=data)\n",
    "\n",
    "        # Update network parameters.\n",
    "        optimiser.zero_grad()\n",
    "        loss = binary_cross_entropy_with_logits(input=model_output, target=target, reduction=r'mean')\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(parameters=network.parameters(), max_norm=1.0, norm_type=2)\n",
    "        optimiser.step()\n",
    "\n",
    "        # Compute performance measures of current model.\n",
    "        accuracy = (model_output.sigmoid().round() == target).to(dtype=torch.float32).mean()\n",
    "        accuracies.append(accuracy.detach().item())\n",
    "        losses.append(loss.detach().item())\n",
    "    \n",
    "    # Report progress of training procedure.\n",
    "    return (sum(losses) / len(losses), sum(accuracies) / len(accuracies))\n",
    "\n",
    "\n",
    "def eval_iter(network: Module,\n",
    "              data_loader: DataLoader\n",
    "             ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the current model.\n",
    "    \n",
    "    :param network: network instance to evaluate\n",
    "    :param data_loader: data loader instance providing validation data\n",
    "    :return: tuple comprising validation loss as well as accuracy\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, accuracies = [], []\n",
    "        for sample_data in data_loader:\n",
    "            data, target = sample_data[r'data'], sample_data[r'target']\n",
    "            data, target = data.to(device=device), target.to(device=device)\n",
    "\n",
    "            # Process data by Hopfield-based network.\n",
    "            model_output = network.forward(input=data)\n",
    "            loss = binary_cross_entropy_with_logits(input=model_output, target=target, reduction=r'mean')\n",
    "\n",
    "            # Compute performance measures of current model.\n",
    "            accuracy = (model_output.sigmoid().round() == target).to(dtype=torch.float32).mean()\n",
    "            accuracies.append(accuracy.detach().item())\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "        # Report progress of validation procedure.\n",
    "        return (sum(losses) / len(losses), sum(accuracies) / len(accuracies))\n",
    "\n",
    "\n",
    "def operate(network: Module,\n",
    "            optimiser: AdamW,\n",
    "            data_loader_train: DataLoader,\n",
    "            data_loader_eval: DataLoader,\n",
    "            num_epochs: int = 1\n",
    "           ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Train the specified network by gradient descent using backpropagation.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader_train: data loader instance providing training data\n",
    "    :param data_loader_eval: data loader instance providing validation data\n",
    "    :param num_epochs: amount of epochs to train\n",
    "    :return: data frame comprising training as well as evaluation performance\n",
    "    \"\"\"\n",
    "    losses, accuracies = {r'train': [], r'eval': []}, {r'train': [], r'eval': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train network.\n",
    "        performance = train_epoch(network, optimiser, data_loader_train)\n",
    "        losses[r'train'].append(performance[0])\n",
    "        accuracies[r'train'].append(performance[1])\n",
    "        \n",
    "        # Evaluate current model.\n",
    "        performance = eval_iter(network, data_loader_eval)\n",
    "        losses[r'eval'].append(performance[0])\n",
    "        accuracies[r'eval'].append(performance[1])\n",
    "    \n",
    "    # Report progress of training and validation procedures.\n",
    "    return pd.DataFrame(losses), pd.DataFrame(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set seed for all underlying (pseudo) random number sources.\n",
    "    \n",
    "    :param seed: seed to be used\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def plot_performance(loss: pd.DataFrame,\n",
    "                     accuracy: pd.DataFrame,\n",
    "                     log_file: str\n",
    "                    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot and save loss and accuracy.\n",
    "    \n",
    "    :param loss: loss to be plotted\n",
    "    :param accuracy: accuracy to be plotted\n",
    "    :param log_file: target file for storing the resulting plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    \n",
    "    loss_plot = sns.lineplot(data=loss, ax=ax[0])\n",
    "    loss_plot.set(xlabel=r'Epoch', ylabel=r'Cross-entropy Loss')\n",
    "    \n",
    "    accuracy_plot = sns.lineplot(data=accuracy, ax=ax[1])\n",
    "    accuracy_plot.set(xlabel=r'Epoch', ylabel=r'Accuracy')\n",
    "    \n",
    "    ax[1].yaxis.set_label_position(r'right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(log_file)\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LSTM-based Network</h2>\n",
    "\n",
    "The instantiation of the heart of an LSTM-based network, the module <code>LSTM</code>, is rather straightforward. Only <i>two</i> arguments, the size of the input as well as the site of the hidden state, need to be set.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>input_size</code></th>\n",
    "        <th>num_characters (20)</th>\n",
    "        <th>Size (depth) of the input.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>hidden_size</code></th>\n",
    "        <th>4</th>\n",
    "        <th>Size (depth) of the hidden state.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this example.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "An additional output projection is defined, to downproject the hidden state of the last time step of the <code>LSTM</code> to the correct output size. Afterwards, everything is wrapped into a container of type <code>torch.nn.Sequential</code> and a corresponding optimiser is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of an LSTM-based network.\n",
    "        \n",
    "        :param input size: size (depth) of the input\n",
    "        :param hidden_size: size (depth) of the hidden state\n",
    "        \"\"\"\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.lstm = LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.projection = Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute result of LSTM-based network on specified data.\n",
    "        \n",
    "        :param input: data to be processed by the LSTM-based network\n",
    "        :return: result as computed by the LSTM-based network\n",
    "        \"\"\"\n",
    "        out, _ = self.lstm.forward(input=input)     \n",
    "        return self.projection.forward(input=out[:, -1, :]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "network = LSTMNetwork(\n",
    "    input_size=latch_sequence_set.num_characters,\n",
    "    hidden_size=4).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate LSTM-based Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/lstm_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hopfield-based Network</h2>\n",
    "\n",
    "The instantiation of the heart of a Hopfield-based network, the module <code>Hopfield</code>, is even simpler. Only <i>one</i> argument, the size of the input, needs to be set.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>input_size</code></th>\n",
    "        <th>num_characters (20)</th>\n",
    "        <th>Size (depth) of the input (state pattern).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this example.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "An additional output projection is defined, to downproject the result of <code>Hopfield</code> to the correct output size. Afterwards, everything is wrapped into a container of type <code>torch.nn.Sequential</code> and a corresponding optimiser is defined. Now the Hopfield-based network and all auxiliaries are set up and ready to <i>associate</i>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield = Hopfield(\n",
    "    input_size=latch_sequence_set.num_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield.output_size * latch_sequence_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield, Flatten(), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate Hopfield-based Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Adapt Hopfield-based Network</h3>\n",
    "<h3 style=\"color:rgb(208,90,80)\">We can now explore the functionality of our Hopfield layer <code>Hopfield</code>.</h3>\n",
    "\n",
    "As described in the paper the Hopfield layer allows:\n",
    "- association of two sets\n",
    "- multiple updates\n",
    "- variable beta\n",
    "- changing the dimension of the associative space\n",
    "- pattern normalization\n",
    "- static patterns for fixed pattern search\n",
    "\n",
    "This time, additional arguments are set to influence the training as well as the validation performance of the Hopfield-based network.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>input_size</code></th>\n",
    "        <th>num_characters (20)</th>\n",
    "        <th>Size (depth) of the input (state pattern).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>hidden_size</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Size (depth) of the association space.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_heads</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Amount of parallel association heads.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>update_steps_max</code></th>\n",
    "        <th>3</th>\n",
    "        <th>Number of updates in one Hopfield head.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>scaling</code></th>\n",
    "        <th>0.25</th>\n",
    "        <th>Beta parameter that determines the kind of fixed point.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>dropout</code></th>\n",
    "        <th>0.5</th>\n",
    "        <th>Dropout probability applied on the association matrix.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this example.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield = Hopfield(\n",
    "    input_size=latch_sequence_set.num_characters,\n",
    "    hidden_size=8,\n",
    "    num_heads=8,\n",
    "    update_steps_max=3,\n",
    "    scaling=0.25,\n",
    "    dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield.output_size * latch_sequence_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield, Flatten(), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_adapted.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hopfield-based Pooling</h2>\n",
    "\n",
    "The previous examples manually downprojected the result of <code>Hopfield</code> by applying a linear layer afterwards. It would've also been possible to apply some kind of <i>pooling</i>. Exactly for <i>such</i> use cases, the module <code>HopfieldPooling</code> might be the right choice. Internally, a <i>state pattern</i> is trained, which in turn is used to compute pooling weights with respect to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_pooling = HopfieldPooling(\n",
    "    input_size=latch_sequence_set.num_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_pooling.output_size, out_features=1)\n",
    "network = Sequential(hopfield_pooling, output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate Hopfield-based Pooling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_pooling.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Adapt Hopfield-based Pooling</h3>\n",
    "<h3 style=\"color:rgb(208,90,80)\">We can now again explore the functionality of our Hopfield-based pooling layer <code>HopfieldPooling</code>.</h3>\n",
    "\n",
    "Again, additional arguments are set to influence the training as well as the validation performance of the Hopfield-based pooling.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>input_size</code></th>\n",
    "        <th>num_characters (20)</th>\n",
    "        <th>Size (depth) of the input (state pattern).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>hidden_size</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Size (depth) of the association space.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_heads</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Amount of parallel association heads.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <th><code>update_steps_max</code></th>\n",
    "        <th>3</th>\n",
    "        <th>Number of updates in one Hopfield head.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>scaling</code></th>\n",
    "        <th>0.25</th>\n",
    "        <th>Beta parameter that determines the kind of fixed point.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>dropout</code></th>\n",
    "        <th>0.5</th>\n",
    "        <th>Dropout probability applied on the association matrix.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this example.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_pooling = HopfieldPooling(\n",
    "    input_size=latch_sequence_set.num_characters,\n",
    "    hidden_size=8,\n",
    "    num_heads=8,\n",
    "    update_steps_max=3,\n",
    "    scaling=0.25,\n",
    "    dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_pooling.output_size, out_features=1)\n",
    "network = Sequential(hopfield_pooling, output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_pooling_adapted.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hopfield-based Lookup</h2>\n",
    "\n",
    "In contrast to the first <code>Hopfield</code> setting, in which the <i>state patterns</i> as well as the <i>stored patterns</i> are directly dependent on the input, <code>HopfieldLayer</code> employs a trainable but fixed <i>stored pattern</i> matrix, which in turn acts as a learnable lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latch_samples_unique = [_[r'data'] for _ in data_loader_train]\n",
    "latch_samples_unique = torch.cat(latch_samples_unique).view(-1, latch_samples_unique[0].shape[2]).unique(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_lookup = HopfieldLayer(\n",
    "    input_size=latch_sequence_set.num_characters,\n",
    "    quantity=len(latch_samples_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_lookup.output_size * latch_sequence_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield_lookup, Flatten(start_dim=1), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate Hopfield-based Lookup</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_lookup.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Adapt Hopfield-based Lookup</h3>\n",
    "<h3 style=\"color:rgb(208,90,80)\">We can now again explore the functionality of our Hopfield-based lookup layer <code>HopfieldLayer</code>.</h3>\n",
    "\n",
    "This <i>lookup setting</i> is especially pronounced, if the <i>state patterns</i> are initialized with a subset of the training set (and optionally provide the corresponding training targets as <i>pattern projection</i> inputs).\n",
    "\n",
    "Again, additional arguments are set to increase the training as well as the validation performance of the Hopfield-based lookup.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>lookup_weights_as_separated</code></th>\n",
    "        <th>True</th>\n",
    "        <th>Separate lookup weights from lookup target weights (e.g. to set lookup target weights separately).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>lookup_targets_as_trainable</code></th>\n",
    "        <th>False</th>\n",
    "        <th>Employ trainable lookup target weights (used as pattern projection input).</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_lookup = HopfieldLayer(\n",
    "    input_size=latch_sequence_set.num_characters,\n",
    "    quantity=len(latch_samples_unique),\n",
    "    lookup_weights_as_separated=True,\n",
    "    lookup_targets_as_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the trainable but fixed <i>stored patterns</i> with all unique samples from the training set. In this way, the Hopfield-based lookup already starts with <i>meaningful</i> stored patterns (instead of random noise). This may enhance the performance of the network, especially at the beginning of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hopfield_lookup.lookup_weights[:] = latch_samples_unique.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_lookup.output_size * latch_sequence_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield_lookup, Flatten(start_dim=1), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_lookup_adapted.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
