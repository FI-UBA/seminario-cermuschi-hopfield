{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example: Attention-based Deep Multiple Instance Learning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general modules used for e.g. plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Import Hopfield-specific modules.\n",
    "from hflayers import HopfieldPooling\n",
    "\n",
    "# Import auxiliary modules.\n",
    "from distutils.version import LooseVersion\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Importing PyTorch specific modules.\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Conv2d, Dropout, Linear, MaxPool2d, Module, ReLU, Sequential, Sigmoid\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set plotting style.\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the path of the Attention-based Deep Multiple Instance Learning (ADMIL) repository to the system path in order for Python to find the corresponding modules to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'./AttentionDeepMIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, the corresponding modules\n",
    "- <code>MnistBags</code>\n",
    "- <code>Attention</code>\n",
    "- <code>GatedAttention</code>\n",
    "\n",
    "are imported to the global namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import MnistBags\n",
    "from model import Attention, GatedAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific minimum versions of Python itself as well as of some used modules is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_check = '(\\u2713)' if sys.version_info >= (3, 8) else '(\\u2717)'\n",
    "pytorch_check = '(\\u2713)' if torch.__version__ >= LooseVersion(r'1.5') else '(\\u2717)'\n",
    "\n",
    "print(f'Installed Python version:  {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro} {python_check}')\n",
    "print(f'Installed PyTorch version: {torch.__version__} {pytorch_check}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Create Dataset</h3>\n",
    "\n",
    "The dataset itself falls into the category of <i>binary classification</i> tasks in the domain of <i>Multiple Instance Learning (MIL)</i> problems.\n",
    "The MNIST-bags task was introcuded by Ilse and Tomczak:<br>\n",
    "<cite>Ilse, M., Tomczak, J.M. and Welling, M., 2018. Attention-based deep multiple instance learning. arXiv preprint arXiv:1802.04712.</cite><br><br>\n",
    "Each bag comprises a collection of $28\\times{}28$ grayscale images/instances, whereas each instance is a sequence of pixel values in the range of $[0; 255]$. The amount of instances per pag is drawn from a Gaussian with specified mean and variance. The positive class is defined by the presence of the target number/digit, whereas the negative one by its absence. This demonstration shows, that <code>HopfieldPooling</code> is capable of learning and filtering each bag with respect to the class-defining target number/digit. Defining arguments are:\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>target_number</code></th>\n",
    "        <th>9</th>\n",
    "        <th>Target number/digit defining class affiliation.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>mean_bag_length</code></th>\n",
    "        <th>10</th>\n",
    "        <th>Mean amount of instances per bag.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>var_bag_length</code></th>\n",
    "        <th>2</th>\n",
    "        <th>Variance of amount of instances per bag.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_bag</code></th>\n",
    "        <th>{200; 50}</th>\n",
    "        <th>Amount of samples of the training as well as validation set.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Let's define the dataset using previously mentioned properties as well as a logging directory for storing all auxiliary outputs like performance plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(r'cuda:0' if torch.cuda.is_available() else r'cpu')\n",
    "\n",
    "# Create data loader of training set.\n",
    "data_loader_train = DataLoader(MnistBags(\n",
    "    target_number=9,\n",
    "    mean_bag_length=10,\n",
    "    var_bag_length=2,\n",
    "    num_bag=200,\n",
    "    train=True\n",
    "), batch_size=1, shuffle=True)\n",
    "\n",
    "# Create data loader of validation set.\n",
    "data_loader_eval = DataLoader(MnistBags(\n",
    "    target_number=9,\n",
    "    mean_bag_length=10,\n",
    "    var_bag_length=2,\n",
    "    num_bag=50,\n",
    "    train=False\n",
    "), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f'resources/'\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Create Auxiliaries</h3>\n",
    "\n",
    "Before digging into Hopfield-based networks, a few auxiliary variables and functions need to be defined. This is nothing special with respect to Hopfield-based networks, but rather common preparation work of (almost) every machine learning setting (e.g. definition of a <i>data loader</i> as well as a <i>training loop</i>). We will see, that this comprises the most work of this whole demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network: Module,\n",
    "                optimiser: AdamW,\n",
    "                data_loader: DataLoader\n",
    "               ) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Execute one training epoch.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader: data loader instance providing training data\n",
    "    :return: tuple comprising training loss, training error as well as accuracy\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    losses, errors, accuracies = [], [], []\n",
    "    for data, target in data_loader:\n",
    "        data, target = data.to(device=device), target[0].to(device=device)\n",
    "\n",
    "        # Process data by Hopfield-based network.\n",
    "        loss = network.calculate_objective(data, target)[0]\n",
    "\n",
    "        # Update network parameters.\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(parameters=network.parameters(), max_norm=1.0, norm_type=2)\n",
    "        optimiser.step()\n",
    "\n",
    "        # Compute performance measures of current model.\n",
    "        error, prediction = network.calculate_classification_error(data, target)\n",
    "        accuracy = (prediction == target).to(dtype=torch.float32).mean()\n",
    "        accuracies.append(accuracy.detach().item())\n",
    "        errors.append(error)\n",
    "        losses.append(loss.detach().item())\n",
    "    \n",
    "    # Report progress of training procedure.\n",
    "    return sum(losses) / len(losses), sum(errors) / len(errors), sum(accuracies) / len(accuracies)\n",
    "\n",
    "\n",
    "def eval_iter(network: Module,\n",
    "              data_loader: DataLoader\n",
    "             ) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the current model.\n",
    "    \n",
    "    :param network: network instance to evaluate\n",
    "    :param data_loader: data loader instance providing validation data\n",
    "    :return: tuple comprising validation loss, validation error as well as accuracy\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, errors, accuracies = [], [], []\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device=device), target[0].to(device=device)\n",
    "\n",
    "            # Process data by Hopfield-based network.\n",
    "            loss = network.calculate_objective(data, target)[0]\n",
    "\n",
    "            # Compute performance measures of current model.\n",
    "            error, prediction = network.calculate_classification_error(data, target)\n",
    "            accuracy = (prediction == target).to(dtype=torch.float32).mean()\n",
    "            accuracies.append(accuracy.detach().item())\n",
    "            errors.append(error)\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "        # Report progress of validation procedure.\n",
    "        return sum(losses) / len(losses), sum(errors) / len(errors), sum(accuracies) / len(accuracies)\n",
    "\n",
    "    \n",
    "def operate(network: Module,\n",
    "            optimiser: AdamW,\n",
    "            data_loader_train: DataLoader,\n",
    "            data_loader_eval: DataLoader,\n",
    "            num_epochs: int = 1\n",
    "           ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Train the specified network by gradient descent using backpropagation.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader_train: data loader instance providing training data\n",
    "    :param data_loader_eval: data loader instance providing validation data\n",
    "    :param num_epochs: amount of epochs to train\n",
    "    :return: data frame comprising training as well as evaluation performance\n",
    "    \"\"\"\n",
    "    losses, errors, accuracies = {r'train': [], r'eval': []}, {r'train': [], r'eval': []}, {r'train': [], r'eval': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train network.\n",
    "        performance = train_epoch(network, optimiser, data_loader_train)\n",
    "        losses[r'train'].append(performance[0])\n",
    "        errors[r'train'].append(performance[1])\n",
    "        accuracies[r'train'].append(performance[2])\n",
    "        \n",
    "        # Evaluate current model.\n",
    "        performance = eval_iter(network, data_loader_eval)\n",
    "        losses[r'eval'].append(performance[0])\n",
    "        errors[r'eval'].append(performance[1])\n",
    "        accuracies[r'eval'].append(performance[2])\n",
    "    \n",
    "    # Report progress of training and validation procedures.\n",
    "    return pd.DataFrame(losses), pd.DataFrame(errors), pd.DataFrame(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set seed for all underlying (pseudo) random number sources.\n",
    "    \n",
    "    :param seed: seed to be used\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def plot_performance(loss: pd.DataFrame,\n",
    "                     error: pd.DataFrame,\n",
    "                     accuracy: pd.DataFrame,\n",
    "                     log_file: str\n",
    "                    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot and save loss and accuracy.\n",
    "    \n",
    "    :param loss: loss to be plotted\n",
    "    :param error: error to be plotted\n",
    "    :param accuracy: accuracy to be plotted\n",
    "    :param log_file: target file for storing the resulting plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    \n",
    "    loss_plot = sns.lineplot(data=loss, ax=ax[0])\n",
    "    loss_plot.set(xlabel=r'Epoch', ylabel=r'Loss')\n",
    "    \n",
    "    error_plot = sns.lineplot(data=error, ax=ax[1])\n",
    "    error_plot.set(xlabel=r'Epoch', ylabel=r'Error')\n",
    "    \n",
    "    accuracy_plot = sns.lineplot(data=accuracy, ax=ax[2])\n",
    "    accuracy_plot.set(xlabel=r'Epoch', ylabel=r'Accuracy')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(log_file)\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Attention-based Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "network = Attention().to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate Attention-based Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses, errors, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, error=errors, accuracy=accuracies, log_file=f'{log_dir}/attention_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GatedAttention-based Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "network = GatedAttention().to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate GatedAttention-based Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses, errors, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, error=errors, accuracy=accuracies, log_file=f'{log_dir}/gated_attention_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hopfield-based Pooling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HfPooling(Module):    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of a Hopfield-based pooling network.\n",
    "        \n",
    "        Note: all hyperparameters of the network are fixed for demonstration purposes.\n",
    "        Morevover, most of the notation of the original implementation is kept in order\n",
    "        to be easier comparable (partially ignoring PEP8).\n",
    "        \"\"\"\n",
    "        super(HfPooling, self).__init__()\n",
    "        self.L = 500\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "\n",
    "        self.feature_extractor_part1 = Sequential(\n",
    "            Conv2d(1, 20, kernel_size=5),\n",
    "            ReLU(),\n",
    "            MaxPool2d(2, stride=2),\n",
    "            Conv2d(20, 50, kernel_size=5),\n",
    "            ReLU(),\n",
    "            MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        self.feature_extractor_part2 = Sequential(\n",
    "            Linear(50 * 4 * 4, self.L),\n",
    "            ReLU(),\n",
    "        )\n",
    "        self.hopfield_pooling = HopfieldPooling(\n",
    "            input_size=self.L, hidden_size=32, output_size=self.L, num_heads=1\n",
    "        )\n",
    "        self.dp = Dropout(\n",
    "            p=0.1\n",
    "        )\n",
    "        self.classifier = Sequential(\n",
    "            Linear(self.L * self.K, 1),\n",
    "            Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tuple[Tensor, Tensor, Optional[Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute result of Hopfield-based pooling network on specified data.\n",
    "        \n",
    "        :param input: data to be processed by the Hopfield-based pooling network\n",
    "        :return: result as computed by the Hopfield-based pooling network\n",
    "        \"\"\"\n",
    "        x = input.squeeze(0)\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        H = H.view(-1, 50 * 4 * 4)\n",
    "        H = self.feature_extractor_part2(H)\n",
    "        \n",
    "        H = H.unsqueeze(0)\n",
    "        H = self.hopfield_pooling(H)\n",
    "        H = H.squeeze(0)\n",
    "        H = self.dp(H)\n",
    "\n",
    "        Y_prob = self.classifier(H)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, None\n",
    "\n",
    "    def calculate_classification_error(self, input: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Compute classification error of current model.\n",
    "        \n",
    "        :param input: data to be processed by the Hopfield-based pooling network\n",
    "        :param target: target to be used to compute the classification error of the current model\n",
    "        :return: classification error as well as predicted class\n",
    "        \"\"\"\n",
    "        Y = target.float()\n",
    "        _, Y_hat, _ = self.forward(input)\n",
    "        error = 1.0 - Y_hat.eq(Y).cpu().float().mean().item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, input: Tensor, target: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute objective of the current model.\n",
    "        \n",
    "        :param input: data to be processed by the Hopfield-based pooling network\n",
    "        :param target: target to be used to compute the objective of the current model\n",
    "        :return: objective as well as dummy A (see accompanying paper for more information)\n",
    "        \"\"\"\n",
    "        Y = target.float()\n",
    "        Y_prob, _, A = self.forward(input)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=(1.0 - 1e-5))\n",
    "        neg_log_likelihood = -1.0 * (Y * torch.log(Y_prob) + (1.0 - Y) * torch.log(1.0 - Y_prob))\n",
    "\n",
    "        return neg_log_likelihood, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "network = HfPooling().to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate HopfieldPooling-based Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses, errors, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, error=errors, accuracy=accuracies, log_file=f'{log_dir}/hopfield_pooling.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
