{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example: Bit Pattern Set</h1>\n",
    "\n",
    "The dataset of this demonstration falls into the category of <i>binary classification</i> tasks in the domain of <i>Multiple Instance Learning (MIL)</i> problems. Each bag comprises a collection of bit pattern instances, wheres each instance is a sequence of <b>0s</b> and <b>1s</b>. The positive class has specific bit patterns injected, which are absent in the negative one. This demonstration shows, that <code>Hopfield</code>, <code>HopfieldPooling</code> and <code>HopfieldLayer</code> are capable of learning and filtering each bag with respect to the class-defining bit patterns.\n",
    "\n",
    "This demonstration instructs how to apply <code>Hopfield</code>, <code>HopfieldPooling</code> and <code>HopfieldLayer</code> for an exemplary Multiple Instance Learning problem.\n",
    "\n",
    "<h3 style=\"color:rgb(208,90,80)\">In the chapters <a href=\"#Adapt-Hopfield-based-Network\">Adapt Hopfield-based Network</a>, <a href=\"#Adapt-Hopfield-based-Pooling\">Adapt Hopfield-based Pooling</a> and <a href=\"#Adapt-Hopfield-based-Lookup\">Adapt Hopfield-based Lookup</a> you can explore and try the new functionalities of our new Hopfield layer.</h3>\n",
    "\n",
    "In order to run this notebook, a few modules need to be imported. The installation of third-party modules is <i>not</i> covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general modules used e.g. for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Importing Hopfield-specific modules.\n",
    "from hflayers import Hopfield, HopfieldPooling, HopfieldLayer\n",
    "from hflayers.auxiliary.data import BitPatternSet\n",
    "\n",
    "# Import auxiliary modules.\n",
    "from distutils.version import LooseVersion\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Importing PyTorch specific modules.\n",
    "from torch import Tensor\n",
    "from torch.nn import Flatten, Linear, Module, Sequential\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Set plotting style.\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific minimum versions of Python itself as well as of some used modules is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_check = '(\\u2713)' if sys.version_info >= (3, 8) else '(\\u2717)'\n",
    "pytorch_check = '(\\u2713)' if torch.__version__ >= LooseVersion(r'1.5') else '(\\u2717)'\n",
    "\n",
    "print(f'Installed Python version:  {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro} {python_check}')\n",
    "print(f'Installed PyTorch version: {torch.__version__} {pytorch_check}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Create Dataset</h3>\n",
    "\n",
    "The dataset itself falls into the category of <i>binary classification</i> tasks in the domain of <i>Multiple Instance Learning (MIL)</i> problems. Defining arguments are:\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_bags</code></th>\n",
    "        <th>2048</th>\n",
    "        <th>Amount of bags (samples) of the full dataset.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_instances</code></th>\n",
    "        <th>16</th>\n",
    "        <th>Amount of instances <i>per</i> bag (sample).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_signals</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Amount of unique instances indicative for the positive class.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_signals_per_bag</code></th>\n",
    "        <th>1</th>\n",
    "        <th>Amount of signals implanted into one bag of a positive class.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_bits</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Amount of \"bits\" (feature size) per instance.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this demo.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Let's define the dataset using previously mentioned properties as well as a logging directory for storing all auxiliary outputs like performance plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_pattern_set = BitPatternSet(\n",
    "    num_bags=2048,\n",
    "    num_instances=16,\n",
    "    num_signals=8,\n",
    "    num_signals_per_bag=1,\n",
    "    num_bits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f'resources/'\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Create Auxiliaries</h3>\n",
    "\n",
    "Before digging into Hopfield-based networks, a few auxiliary variables and functions need to be defined. This is nothing special with respect to Hopfield-based networks, but rather common preparation work of (almost) every machine learning setting (e.g. definition of a <i>data loader</i> as well as a <i>training loop</i>). We will see, that this comprises the most work of this whole demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(r'cuda:0' if torch.cuda.is_available() else r'cpu')\n",
    "\n",
    "# Create data loader of training set.\n",
    "sampler_train = SubsetRandomSampler(list(range(256, 2048 - 256)))\n",
    "data_loader_train = DataLoader(dataset=bit_pattern_set, batch_size=32, sampler=sampler_train)\n",
    "\n",
    "# Create data loader of validation set.\n",
    "sampler_eval = SubsetRandomSampler(list(range(256)) + list(range(2048 - 256, 2048)))\n",
    "data_loader_eval = DataLoader(dataset=bit_pattern_set, batch_size=32, sampler=sampler_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network: Module,\n",
    "                optimiser: AdamW,\n",
    "                data_loader: DataLoader\n",
    "               ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Execute one training epoch.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader: data loader instance providing training data\n",
    "    :return: tuple comprising training loss as well as accuracy\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    losses, accuracies = [], []\n",
    "    for sample_data in data_loader:\n",
    "        data, target = sample_data[r'data'], sample_data[r'target']\n",
    "        data, target = data.to(device=device), target.to(device=device)\n",
    "\n",
    "        # Process data by Hopfield-based network.\n",
    "        model_output = network.forward(input=data)\n",
    "\n",
    "        # Update network parameters.\n",
    "        optimiser.zero_grad()\n",
    "        loss = binary_cross_entropy_with_logits(input=model_output, target=target, reduction=r'mean')\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(parameters=network.parameters(), max_norm=1.0, norm_type=2)\n",
    "        optimiser.step()\n",
    "\n",
    "        # Compute performance measures of current model.\n",
    "        accuracy = (model_output.sigmoid().round() == target).to(dtype=torch.float32).mean()\n",
    "        accuracies.append(accuracy.detach().item())\n",
    "        losses.append(loss.detach().item())\n",
    "    \n",
    "    # Report progress of training procedure.\n",
    "    return (sum(losses) / len(losses), sum(accuracies) / len(accuracies))\n",
    "\n",
    "\n",
    "def eval_iter(network: Module,\n",
    "              data_loader: DataLoader\n",
    "             ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the current model.\n",
    "    \n",
    "    :param network: network instance to evaluate\n",
    "    :param data_loader: data loader instance providing validation data\n",
    "    :return: tuple comprising validation loss as well as accuracy\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, accuracies = [], []\n",
    "        for sample_data in data_loader:\n",
    "            data, target = sample_data[r'data'], sample_data[r'target']\n",
    "            data, target = data.to(device=device), target.to(device=device)\n",
    "\n",
    "            # Process data by Hopfield-based network.\n",
    "            model_output = network.forward(input=data)\n",
    "            loss = binary_cross_entropy_with_logits(input=model_output, target=target, reduction=r'mean')\n",
    "\n",
    "            # Compute performance measures of current model.\n",
    "            accuracy = (model_output.sigmoid().round() == target).to(dtype=torch.float32).mean()\n",
    "            accuracies.append(accuracy.detach().item())\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "        # Report progress of validation procedure.\n",
    "        return (sum(losses) / len(losses), sum(accuracies) / len(accuracies))\n",
    "\n",
    "\n",
    "def operate(network: Module,\n",
    "            optimiser: AdamW,\n",
    "            data_loader_train: DataLoader,\n",
    "            data_loader_eval: DataLoader,\n",
    "            num_epochs: int = 1\n",
    "           ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Train the specified network by gradient descent using backpropagation.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader_train: data loader instance providing training data\n",
    "    :param data_loader_eval: data loader instance providing validation data\n",
    "    :param num_epochs: amount of epochs to train\n",
    "    :return: data frame comprising training as well as evaluation performance\n",
    "    \"\"\"\n",
    "    losses, accuracies = {r'train': [], r'eval': []}, {r'train': [], r'eval': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train network.\n",
    "        performance = train_epoch(network, optimiser, data_loader_train)\n",
    "        losses[r'train'].append(performance[0])\n",
    "        accuracies[r'train'].append(performance[1])\n",
    "        \n",
    "        # Evaluate current model.\n",
    "        performance = eval_iter(network, data_loader_eval)\n",
    "        losses[r'eval'].append(performance[0])\n",
    "        accuracies[r'eval'].append(performance[1])\n",
    "    \n",
    "    # Report progress of training and validation procedures.\n",
    "    return pd.DataFrame(losses), pd.DataFrame(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set seed for all underlying (pseudo) random number sources.\n",
    "    \n",
    "    :param seed: seed to be used\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def plot_performance(loss: pd.DataFrame,\n",
    "                     accuracy: pd.DataFrame,\n",
    "                     log_file: str\n",
    "                    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot and save loss and accuracy.\n",
    "    \n",
    "    :param loss: loss to be plotted\n",
    "    :param accuracy: accuracy to be plotted\n",
    "    :param log_file: target file for storing the resulting plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    \n",
    "    loss_plot = sns.lineplot(data=loss, ax=ax[0])\n",
    "    loss_plot.set(xlabel=r'Epoch', ylabel=r'Cross-entropy Loss')\n",
    "    \n",
    "    accuracy_plot = sns.lineplot(data=accuracy, ax=ax[1])\n",
    "    accuracy_plot.set(xlabel=r'Epoch', ylabel=r'Accuracy')\n",
    "    \n",
    "    ax[1].yaxis.set_label_position(r'right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(log_file)\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hopfield-based Network</h2>\n",
    "\n",
    "The instantiation of the heart of a Hopfield-based network, the module <code>Hopfield</code>, is rather straightforward. Only <i>one</i> argument, the size of the input, needs to be set.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>input_size</code></th>\n",
    "        <th>num_bits (8)</th>\n",
    "        <th>Size (depth) of the input (state pattern).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this example.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "An additional output projection is defined, to downproject the result of <code>Hopfield</code> to the correct output size. Afterwards, everything is wrapped into a container of type <code>torch.nn.Sequential</code> and a corresponding optimiser is defined. Now the Hopfield-based network and all auxiliaries are set up and ready to <i>associate</i>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield = Hopfield(\n",
    "    input_size=bit_pattern_set.num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield.output_size * bit_pattern_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield, Flatten(), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate Hopfield-based Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Adapt Hopfield-based Network</h3>\n",
    "<h3 style=\"color:rgb(208,90,80)\">We can now explore the functionality of our Hopfield layer <code>Hopfield</code>.</h3>\n",
    "\n",
    "As described in the paper the Hopfield layer allows:\n",
    "- association of two sets\n",
    "- multiple updates\n",
    "- variable beta\n",
    "- changing the dimension of the associative space\n",
    "- pattern normalization\n",
    "- static patterns for fixed pattern search\n",
    "\n",
    "This time, additional arguments are set to increase the training as well as the validation performance of the Hopfield-based network.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>input_size</code></th>\n",
    "        <th>num_bits (8)</th>\n",
    "        <th>Size (depth) of the input (state pattern).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>hidden_size</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Size (depth) of the association space.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_heads</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Amount of parallel association heads.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>update_steps_max</code></th>\n",
    "        <th>3</th>\n",
    "        <th>Number of updates in one Hopfield head.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>scaling</code></th>\n",
    "        <th>0.25</th>\n",
    "        <th>Beta parameter that determines the kind of fixed point.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>dropout</code></th>\n",
    "        <th>0.5</th>\n",
    "        <th>Dropout probability applied on the association matrix.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this example.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield = Hopfield(\n",
    "    input_size=bit_pattern_set.num_bits,\n",
    "    hidden_size=8,\n",
    "    num_heads=8,\n",
    "    update_steps_max=3,\n",
    "    scaling=0.25,\n",
    "    dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield.output_size * bit_pattern_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield, Flatten(), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_adapted.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hopfield-based Pooling</h2>\n",
    "\n",
    "The previous examples manually downprojected the result of <code>Hopfield</code> by applying a linear layer afterwards. It would've also been possible to apply some kind of <i>pooling</i>. Exactly for <i>such</i> use cases, the module <code>HopfieldPooling</code> might be the right choice. Internally, a <i>state pattern</i> is trained, which in turn is used to compute pooling weights with respect to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_pooling = HopfieldPooling(\n",
    "    input_size=bit_pattern_set.num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_pooling.output_size, out_features=1)\n",
    "network = Sequential(hopfield_pooling, output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate Hopfield-based Pooling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_pooling.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Adapt Hopfield-based Pooling</h3>\n",
    "<h3 style=\"color:rgb(208,90,80)\">We can now again explore the functionality of our Hopfield-based pooling layer <code>HopfieldPooling</code>.</h3>\n",
    "\n",
    "Again, additional arguments are set to increase the training as well as the validation performance of the Hopfield-based pooling.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>input_size</code></th>\n",
    "        <th>num_bits (8)</th>\n",
    "        <th>Size (depth) of the input (state pattern).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>hidden_size</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Size (depth) of the association space.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_heads</code></th>\n",
    "        <th>8</th>\n",
    "        <th>Amount of parallel association heads.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <th><code>update_steps_max</code></th>\n",
    "        <th>3</th>\n",
    "        <th>Number of updates in one Hopfield head.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>scaling</code></th>\n",
    "        <th>0.25</th>\n",
    "        <th>Beta parameter that determines the kind of fixed point.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>dropout</code></th>\n",
    "        <th>0.5</th>\n",
    "        <th>Dropout probability applied on the association matrix.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>...</code></th>\n",
    "        <th>default</th>\n",
    "        <th>The remaining arguments are not explicitly used in this example.</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_pooling = HopfieldPooling(\n",
    "    input_size=bit_pattern_set.num_bits,\n",
    "    hidden_size=8,\n",
    "    num_heads=8,\n",
    "    update_steps_max=3,\n",
    "    scaling=0.25,\n",
    "    dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_pooling.output_size, out_features=1)\n",
    "network = Sequential(hopfield_pooling, output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_pooling_adapted.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hopfield-based Lookup</h2>\n",
    "\n",
    "In contrast to the first <code>Hopfield</code> setting, in which the <i>state patterns</i> as well as the <i>stored patterns</i> are directly dependent on the input, <code>HopfieldLayer</code> employs a trainable but fixed <i>stored pattern</i> matrix, which in turn acts as a learnable lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_samples_unique = [_[r'data'] for _ in data_loader_train]\n",
    "bit_samples_unique = torch.cat(bit_samples_unique).view(-1, bit_samples_unique[0].shape[2]).unique(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_lookup = HopfieldLayer(\n",
    "    input_size=bit_pattern_set.num_bits,\n",
    "    quantity=len(bit_samples_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_lookup.output_size * bit_pattern_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield_lookup, Flatten(start_dim=1), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Operate Hopfield-based Lookup</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_lookup.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Adapt Hopfield-based Lookup</h3>\n",
    "<h3 style=\"color:rgb(208,90,80)\">We can now again explore the functionality of our Hopfield-based lookup layer <code>HopfieldLayer</code>.</h3>\n",
    "\n",
    "This <i>lookup setting</i> is especially pronounced, if the <i>state patterns</i> are initialized with a subset of the training set (and optionally provide the corresponding training targets as <i>pattern projection</i> inputs).\n",
    "\n",
    "Again, additional arguments are set to increase the training as well as the validation performance of the Hopfield-based lookup.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this demo)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>lookup_weights_as_separated</code></th>\n",
    "        <th>True</th>\n",
    "        <th>Separate lookup weights from lookup target weights (e.g. to set lookup target weights separately).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>lookup_targets_as_trainable</code></th>\n",
    "        <th>False</th>\n",
    "        <th>Employ trainable lookup target weights (used as pattern projection input).</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "hopfield_lookup = HopfieldLayer(\n",
    "    input_size=bit_pattern_set.num_bits,\n",
    "    quantity=len(bit_samples_unique),\n",
    "    lookup_weights_as_separated=True,\n",
    "    lookup_targets_as_trainable=False,\n",
    "    normalize_stored_pattern_affine=True,\n",
    "    normalize_pattern_projection_affine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the trainable but fixed <i>stored patterns</i> with all unique samples from the training set. In this way, the Hopfield-based lookup already starts with <i>meaningful</i> stored patterns (instead of random noise). This may enhance the performance of the network, especially at the beginning of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hopfield_lookup.lookup_weights[:] = bit_samples_unique.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = Linear(in_features=hopfield_lookup.output_size * bit_pattern_set.num_instances, out_features=1)\n",
    "network = Sequential(hopfield_lookup, Flatten(start_dim=1), output_projection, Flatten(start_dim=0)).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_lookup_adapted.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
