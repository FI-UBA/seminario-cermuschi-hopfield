{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code illustrates the learning algorithm for Dense Associative Memories from [Dense Associative Memory for Pattern Recognition](https://arxiv.org/abs/1606.01164) on MNIST data set.\n",
    "If you want to learn more about Dense Associative Memories, check out a [NIPS 2016 talk](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Dense-Associative-Memory-for-Pattern-Recognition) or a [research seminar](https://www.youtube.com/watch?v=lvuAU_3t134). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the data and normalizes it to the [-1,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MAT File\n",
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "# mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size = 28x28 pixels\n",
    "N=784\n",
    "# Number of classes\n",
    "Nc=3\n",
    "# Number of Training samples\n",
    "#Ns=Nc*6000\n",
    "# Ns=24754\n",
    "Ns=3000\n",
    "# Number of Testing samples\n",
    "NsT=3000\n",
    "\n",
    "# Row vector\n",
    "M=np.zeros((0,N))\n",
    "# Column vector\n",
    "Lab=np.zeros((Nc,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Nc):\n",
    "    M=np.concatenate((M, mat['train'+str(i)]), axis=0)\n",
    "    lab1=-np.ones((Nc,mat['train'+str(i)].shape[0]))\n",
    "    lab1[i,:]=1.0\n",
    "    Lab=np.concatenate((Lab,lab1), axis=1)\n",
    "M=2*M/255.0-1\n",
    "M=M.T    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forzando una imagen por clase\n",
    "M=np.array([M[:, 0], M[:, 6000], M[:, 16000]]).T\n",
    "Lab=np.array([[1.0, -1.0, -1.0], [-1.0, 1.0, -1.0], [-1.0, -1.0, 1.0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=np.tile(M, (1, 1000))\n",
    "Lab=np.tile(Lab, (1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.shape)\n",
    "digito = M[:, 2].reshape((28,28))\n",
    "digito\n",
    "plt.imshow(digito,cmap='bwr',vmin=-1,vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MT=np.zeros((0,N))\n",
    "LabT=np.zeros((Nc,0))\n",
    "for i in range(Nc):\n",
    "    MT=np.concatenate((MT, mat['test'+str(i)]), axis=0)\n",
    "    lab1=-np.ones((Nc,mat['test'+str(i)].shape[0]))\n",
    "    lab1[i,:]=1.0\n",
    "    LabT=np.concatenate((LabT,lab1), axis=1)\n",
    "MT=2*MT/255.0-1\n",
    "MT=MT.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forzando una imagen por clase\n",
    "MT=np.array([MT[:, 0], MT[:, 1000], MT[:, 2150]]).T\n",
    "LabT=np.array([[1.0, -1.0, -1.0], [-1.0, 1.0, -1.0], [-1.0, -1.0, 1.0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MT=np.tile(MT, (1, 1000))\n",
    "LabT=np.tile(LabT, (1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MT.shape)\n",
    "digito = MT[:, 2].reshape((28,28))\n",
    "digito\n",
    "plt.imshow(digito,cmap='bwr',vmin=-1,vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a heatmap of the weights together with the errors on the training set (blue) and the test set (red) a helper function is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(synapses, Kx, Ky, err_tr, err_test):\n",
    "    fig.clf()\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    plt.sca(ax1)\n",
    "    yy=0\n",
    "    HM=np.zeros((28*Kx,28*Ky))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*28:(y+1)*28,x*28:(x+1)*28]=synapses[yy,:].reshape(28,28)\n",
    "            yy += 1\n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)\n",
    "    cbar=fig.colorbar(im,ticks=[np.amin(HM), 0, np.amax(HM)])\n",
    "    plt.axis('off')\n",
    "    cbar.ax.tick_params(labelsize=30) \n",
    "    \n",
    "    plt.sca(ax2)\n",
    "    plt.ylim((0,100))\n",
    "    plt.xlim((0,len(err_tr)+1))\n",
    "    ax2.plot(np.arange(1, len(err_tr)+1, 1), err_tr, color='b', linewidth=4)\n",
    "    ax2.plot(np.arange(1, len(err_test)+1, 1), err_test, color='r',linewidth=4)\n",
    "    ax2.set_xlabel('Number of epochs', size=30)\n",
    "    ax2.set_ylabel('Training and test error, %', size=30)\n",
    "    ax2.tick_params(labelsize=30)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines parameters of the algorithm: `n` - power of the rectified polynomial in [Eq 3](https://arxiv.org/abs/1606.01164); `m` - power of the loss function in [Eq 14](https://arxiv.org/abs/1606.01164); `K` - number of memories that are displayed as an `Ky` by `Kx` array by the helper function defined above; `eps0` - initial learning rate that is exponentially annealed during training with the damping parameter `f`, as explained in [Eq 12](https://arxiv.org/abs/1606.01164); `p` - momentum as defined in [Eq 13](https://arxiv.org/abs/1606.01164); `mu` - the mean of the gaussian distribution that initializes the weights; `sigma` - the standard deviation of that gaussian; `Nep` - number of epochs; `Num` - size of the training minibatch; `NumT` - size of the test minibatch; `prec` - parameter that controls numerical precision of the weight updates. Parameter `beta` that is used in [Eq 9](https://arxiv.org/abs/1606.01164) is defined as `beta=1/Temp**n`. The choice of temperatures `Temp` as well as the duration of the annealing `thresh_pret` is discussed in [Appendix A](https://arxiv.org/abs/1606.01164). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kx=2              # Number of memories per row on the weights plot\n",
    "Ky=2              # Number of memories per column on the weigths plot\n",
    "K=Kx*Ky            # Number of memories\n",
    "n=20               # Power of the interaction vertex in the DAM energy function\n",
    "m=30               # Power of the loss function\n",
    "eps0=4.0e-2        # Initial learning rate  \n",
    "f=0.998            # Damping parameter for the learning rate\n",
    "p=0.6              # Momentum\n",
    "Nep=300            # Number of epochs\n",
    "Temp_in=540.       # Initial temperature\n",
    "Temp_f=540.        # Final temperature\n",
    "thresh_pret=200    # Length of the temperature ramp\n",
    "Num=1000           # Size of training minibatch     \n",
    "NumT=1000          # Size of test minibatch \n",
    "mu=-0.3            # Weights initialization mean\n",
    "sigma=0.3          # Weights initialization std\n",
    "prec=1.0e-30       # Precision of weight update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the main code. The external loop runs over epochs `nep`, the internal loop runs over minibatches.  The weights are updated after each minibatch in a way so that the largest update is equal to the learning rate `eps` at that epoch, see [Eq 13](https://arxiv.org/abs/1606.01164). The weights are displayed by the helper function after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(12,10))\n",
    "\n",
    "KS=np.random.normal(mu, sigma, (K, N+Nc))\n",
    "VKS=np.zeros((K, N+Nc))\n",
    "\n",
    "aux=-np.ones((Nc,Num*Nc))\n",
    "for d in range(Nc):\n",
    "    aux[d,d*Num:(d+1)*Num]=1.\n",
    "\n",
    "auxT=-np.ones((Nc,NumT*Nc))\n",
    "for d in range(Nc):\n",
    "    auxT[d,d*NumT:(d+1)*NumT]=1.\n",
    "    \n",
    "err_tr=[]\n",
    "err_test=[]\n",
    "for nep in range(Nep):\n",
    "    eps=eps0*f**nep\n",
    "    # Temperature ramp\n",
    "    if nep<=thresh_pret:\n",
    "        Temp=Temp_in+(Temp_f-Temp_in)*nep/thresh_pret\n",
    "    else:\n",
    "        Temp=Temp_f\n",
    "    beta=1./Temp**n\n",
    "\n",
    "    perm=np.random.permutation(Ns)\n",
    "    M=M[:,perm]\n",
    "    Lab=Lab[:,perm]\n",
    "    num_correct = 0\n",
    "    for k in range(Ns//Num):\n",
    "        v=M[:,k*Num:(k+1)*Num]\n",
    "        t_R=Lab[:,k*Num:(k+1)*Num]\n",
    "        t=np.reshape(t_R,(1,Nc*Num))\n",
    "        \n",
    "        u=np.concatenate((v, -np.ones((Nc,Num))),axis=0)\n",
    "        uu=np.tile(u,(1,Nc))\n",
    "        vv=np.concatenate((uu[:N,:],aux),axis=0)\n",
    "                \n",
    "        KSvv=np.maximum(np.dot(KS,vv),0)\n",
    "        KSuu=np.maximum(np.dot(KS,uu),0)\n",
    "        \n",
    "        Y=np.tanh(beta*np.sum(KSvv**n-KSuu**n, axis=0))  # Forward path, Eq 9\n",
    "        Y_R=np.reshape(Y,(Nc,Num))\n",
    "        \n",
    "        #Gradients of the loss function\n",
    "        d_KS=np.dot(np.tile((t-Y)**(2*m-1)*(1-Y)*(1+Y), (K,1))*KSvv**(n-1),vv.T) - np.dot(np.tile((t-Y)**(2*m-1)*(1-Y)*(1+Y), (K,1))*KSuu**(n-1),uu.T)\n",
    "        \n",
    "        VKS=p*VKS+d_KS\n",
    "        nc=np.amax(np.absolute(VKS),axis=1).reshape(K,1)\n",
    "        nc[nc<prec]=prec\n",
    "        ncc=np.tile(nc,(1,N+Nc))\n",
    "        KS += eps*VKS/ncc\n",
    "        KS=np.clip(KS, a_min=-1., a_max=1.)\n",
    "            \n",
    "        correct=np.argmax(Y_R,axis=0)==np.argmax(t_R,axis=0)\n",
    "        num_correct += np.sum(correct)\n",
    "        \n",
    "    err_tr.append(100.*(1.0-num_correct/Ns))\n",
    "    \n",
    "    num_correct = 0\n",
    "    for k in range(NsT//NumT):\n",
    "        v=MT[:,k*NumT:(k+1)*NumT]\n",
    "        t_R=LabT[:,k*NumT:(k+1)*NumT]\n",
    "        u=np.concatenate((v, -np.ones((Nc,NumT))),axis=0)\n",
    "        uu=np.tile(u,(1,Nc))\n",
    "        vv=np.concatenate((uu[:N,:],auxT),axis=0)\n",
    "        KSvv=np.maximum(np.dot(KS,vv),0)\n",
    "        KSuu=np.maximum(np.dot(KS,uu),0)\n",
    "        Y=np.tanh(beta*np.sum(KSvv**n-KSuu**n, axis=0))  # Forward path, Eq 9\n",
    "        Y_R=np.reshape(Y,(Nc,NumT))\n",
    "        correct=np.argmax(Y_R,axis=0)==np.argmax(t_R,axis=0)\n",
    "        num_correct += np.sum(correct)\n",
    "    errr=100.*(1.0-num_correct/NsT)\n",
    "    err_test.append(errr)\n",
    "    draw_weights(KS[:,:N], Kx, Ky, err_tr, err_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_R[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_R[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
